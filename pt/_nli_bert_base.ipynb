{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882fadcb-828e-40f6-9e3a-9d5c62bed0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import d2l_torch as d2l\n",
    "import json\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc210838-b58d-4bb5-86b7-9fc7f7047769",
   "metadata": {},
   "source": [
    "### jupyter只能跟踪主进程，没法跟踪子进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce1b978-95ee-4048-8c23-f89023d87fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = \"..\\data\\snli_1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2709e8cc-e155-4981-a0a6-b3c1bc0231a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7d9fbe-e67d-469f-aa77-3a35a26d7777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_blks, dropout, max_len, devices):\n",
    "    pretrained_model_dir = d2l.download_extract(pretrained_model)\n",
    "    # Define an empty vocabulary to load the predefined vocabulary\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(pretrained_model_dir, 'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(\n",
    "        len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=num_heads,\n",
    "        num_blks=num_blks, dropout=dropout, max_len=max_len)\n",
    "    # Load pretrained BERT parameters\n",
    "    bert.load_state_dict(torch.load(os.path.join(pretrained_model_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79fbf95-580c-43b1-a550-df6df3c4df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pretrained_model start:  2023-10-04 01:29:33.619583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rongjoker\\anaconda3\\envs\\pandas\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pretrained_model end:  2023-10-04 01:29:38.006520\n"
     ]
    }
   ],
   "source": [
    "    devices = d2l.try_all_gpus()\n",
    "    print('load_pretrained_model start: ', datetime.datetime.now())\n",
    "    bert, vocab = load_pretrained_model(\n",
    "        'bert.base', num_hiddens=768, ffn_num_hiddens=3072, num_heads=12,\n",
    "        num_blks=12, dropout=0.1, max_len=512, devices=devices)\n",
    "    print('load_pretrained_model end: ', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9755a7-c673-44db-8952-224946c5f794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        out = [self._mp_worker(single_premise_hypothesis_tokens) for single_premise_hypothesis_tokens in\n",
    "               all_premise_hypothesis_tokens]\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                    * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7cd0fd8-4a86-44bd-85b3-bfb19760db73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8027648a-a495-4e4c-b85b-9facd4af0fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_bert_fine_tuning(lr= 1e-4, num_epochs=5):\n",
    "    # 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "    # Reduce `batch_size` if there is an out of memory error. In the original BERT\n",
    "    # model, `max_len` = 512\n",
    "    batch_size, max_len, num_workers = 80, 128, d2l.get_dataloader_workers()\n",
    "    # data_dir = d2l.download_extract('SNLI')\n",
    "    print('load dataset start: ', datetime.datetime.now())\n",
    "    train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "    test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            num_workers=num_workers)\n",
    "    print('load dataset end: ', datetime.datetime.now())\n",
    "\n",
    "    net = BERTClassifier(bert)\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    print('train start: ', datetime.datetime.now())\n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "                   devices)\n",
    "    print('train end: ', datetime.datetime.now())\n",
    "    torch.save(net.state_dict(), 'model/nli_bert_base_jupyter.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc6f289-4c92-4107-9d38-737ef86f869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIBERTInferData:\n",
    "    def __init__(self, ss, max_len, vocab=None):\n",
    "        pp = [sentences[0] for sentences in ss]\n",
    "        hh = [sentences[1] for sentences in ss]\n",
    "        kk = [pp, hh]\n",
    "\n",
    "        premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in kk])]\n",
    "        # print(premise_hypothesis_tokens[0])\n",
    "        # [['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], ['a',\n",
    "        # 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.']]\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(premise_hypothesis_tokens)\n",
    "        # print(self.all_token_ids[0])\n",
    "        # print(self.all_segments[0])\n",
    "        # print(self.valid_lens[0])\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        # out = self._mp_worker(all_premise_hypothesis_tokens)\n",
    "        out = [self._mp_worker(single_premise_hypothesis_tokens) for single_premise_hypothesis_tokens in\n",
    "               all_premise_hypothesis_tokens]\n",
    "        # print('out:', type(out))\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long, device=d2l.try_gpu()),\n",
    "                torch.tensor(all_segments, dtype=torch.long, device=d2l.try_gpu()),\n",
    "                torch.tensor(valid_lens, device=d2l.try_gpu()))\n",
    "\n",
    "    def _mp_worker(self, single_premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = single_premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                    * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx])\n",
    "\n",
    "    def get_item(self, idx):\n",
    "        return ([self.all_token_ids[idx]], [self.all_segments[idx]],\n",
    "                [self.valid_lens[idx]])\n",
    "\n",
    "    def get_all_entity(self):\n",
    "        return (self.all_token_ids, self.all_segments,\n",
    "                self.valid_lens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e29f49d-762d-4590-a93b-8c1d69a91e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_test(describe):\n",
    "    # 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "    # Reduce `batch_size` if there is an out of memory error. In the original BERT\n",
    "    # model, `max_len` = 512\n",
    "    # batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "    # # data_dir = d2l.download_extract('SNLI')\n",
    "    # train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "    # test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "    # train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "    #                                          num_workers=num_workers)\n",
    "    # test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "    #                                         num_workers=num_workers)\n",
    "\n",
    "    # 循环前2个\n",
    "    # ss = [sentences for sentences in describe[:2]]\n",
    "    # print(ss)\n",
    "\n",
    "    # for desc in describe:\n",
    "    #     print(infer_snli_bert(desc[0], desc[1]))\n",
    "    #     print('---------------------------------')\n",
    "    # bert, vocab = load_pretrained_model(\n",
    "    #     'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    #     num_blks=2, dropout=0.1, max_len=512, devices=d2l.try_all_gpus())\n",
    "    data = SNLIBERTInferData(ss=describe, max_len=128, vocab=vocab)\n",
    "    model = BERTClassifier(bert)\n",
    "    model.load_state_dict(torch.load('model/nli_bert_base.pth'))\n",
    "    model = model.to(d2l.try_gpu())\n",
    "    model.eval()  # 设置模型为推理模式\n",
    "    # todo tokens_X, segments_X, valid_lens_x = inputs\n",
    "    # X[0].shape: torch.Size([512, 128])\n",
    "    # X[1].shape: torch.Size([512, 128])\n",
    "    # X[2].shape: torch.Size([512])\n",
    "    ret = model(data.get_all_entity())\n",
    "\n",
    "    label = torch.argmax(ret, dim=1)\n",
    "    ss = [('entailment' if l == 0 else 'contradiction' if l == 1 else 'neutral') for l in label]\n",
    "    for i,sentence in enumerate(describe):\n",
    "        print('p:' ,sentence[0])\n",
    "        print('h:' ,sentence[1])\n",
    "        print('l:', ss[i])\n",
    "        print('-----------------------')\n",
    "\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07b0455-e4f1-4612-8e53-e2a9639ae17e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_bert_fine_tuning(num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93e139a5-7046-4312-904f-cb209320f5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 12 examples\n",
      "p: 我是好人\n",
      "h: 我是坏人\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i am a good people\n",
      "h: i am a bad people\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: Two women are hugging each other.\n",
      "h: Two women are showing affection.\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: 两个女人拥抱在一起\n",
      "h: 两个女人在示爱\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i am tired\n",
      "h: i want to sheep\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i am hungry\n",
      "h: i starve\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i am hungry\n",
      "h: i want to eat meal\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i am hungry\n",
      "h: i want to eat meat\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i want to eat meat\n",
      "h: i want to eat meal\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i like gone with the wind\n",
      "h: i like the character scarlet\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i like gone with the wind\n",
      "h: i hate reading\n",
      "l: entailment\n",
      "-----------------------\n",
      "p: i like gone with the wind\n",
      "h: i also like the shortest history of europe\n",
      "l: entailment\n",
      "-----------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment',\n",
       " 'entailment']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = [\n",
    "    ['我是好人', '我是坏人'],\n",
    "    ['i am a good people', 'i am a bad people'],\n",
    "    ['Two women are hugging each other.', 'Two women are showing affection.'],\n",
    "    ['两个女人拥抱在一起', '两个女人在示爱'],\n",
    "    ['i am tired', 'i want to sheep'],\n",
    "    ['i am hungry', 'i starve'],\n",
    "    ['i am hungry', 'i want to eat meal'],\n",
    "    ['i am hungry', 'i want to eat meat'],\n",
    "    ['i want to eat meat', 'i want to eat meal'],\n",
    "    ['i like gone with the wind', 'i like the character scarlet'],\n",
    "    ['i like gone with the wind', 'i hate reading'],\n",
    "    ['i like gone with the wind', 'i also like the shortest history of europe']]\n",
    "infer_test(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558c1a3-a23c-42c3-9fe3-300b99e84d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
